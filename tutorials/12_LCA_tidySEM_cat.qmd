---
title: "LCA with tidySEM (categorical indicators)"
author: "Mauricio Garnier-Villarreal"
date: "`r format(Sys.time(), '%d %B, %Y')`"
format: gfm
toc: true
toc-depth: 5
code-copy: true
---

# Latent Class Analysis (LCA)

Latent class analysis (LCA) is an umbrella term that refers to a number of techniques for estimating unobserved group membership based on a parametric model of one or more observed indicators of group membership. The types of LCA have become quite popular across scientific fields, most notably finite Gaussian mixture modeling and latent profile analysis. Vermunt & Magidson (2004) defined LCA more generally as virtually any statistical model where "some of the parameters [...] differ across unobserved subgroups".

In general terms, we can think of LCA as an **unknown group** analysis, where you think that there is heterogeneity in the data due to differences from these **unknown group**, and we want to first identify these groups, and describe how they are different between each other. This way defining the most homogeneous groups, and heterogeneous between them. 

## Person-centered vs Variable-centered

LCA is part of person-centered methods. Person-centered approaches describe similarities and differences among individuals with respect to how variables relate with each other and are predicated on the assumption that the population is heterogeneous with respect with the relationships between variables. Statistical techniques oriented toward categorizing individuals by patterns of associations among variables, such as LCA and cluster analysis, are person-centered. Variable-centered approaches describe associations among variables and are predicated on the assumption that the population is homogeneous with respect to the relationships between variables. In other words, each association between one variable and another in a variable-centered approach is assumed to hold for all individuals within the population. Statistical techniques oriented toward evaluating the relative importance of predictor variables, such as multivariate regression and structural equation modeling, are variable-centered (Masyn, 2013).

An interesting extension of this, is that any variable-centered approach can be made into a person-centered by defining a model that differs in function of their parameters. For example, we can have a multivariate regression (variable-centered), and we can have a mixture multivariate regression (person-centered). Where for the second we assume that the regression parameters differ across **unknown groups**.

## Terminology

As you might have seen already, there are several terms related to these types of models. Here we have a short list of terms to keep in mind when talking about these models

- Mixture: general terms to denote models that identify **unknown groups** defined by some probabilistic model.
- LCA: latent class analysis, mixture model that defines a categorical latent variable that describes the heterogeneity between groups. Ususally applied only with categorical indicators
- LPA: latent profile analysis, same as LCA, but usually applied with continuous indicators
- Latent variable: an unobserved variable, that cannot be measured with direct items. This variable is the reason people answer in a certain the way the observed indicators. Corrected for (some) measuremet error
- Indicator: observed item that helps approximate the latent variable

You see that there is a distinction between LCA and LPA, but this is historical to the time when software could only estimate models with all indicators being categorical or continuous. Now we can estimate categorical latent variables with categorical, continuous, or a mix of these indicators. For this reason we will use the general term LCA for categorical latent variable model, independent of the type of indicator. 

# `tidySEM`

For this tutorial we will use the package `tidySEM`. Which has the ability to estimate LCAs, with categorical and continuous indicators. This uses `OpenMx` as the back engine that estimates the models. But it provides user friendly functions to estimate, and summarize the models.

Here we first load the packages that we will use, to work with the data, estimate models, and summarize results. 


```{r}
#| echo: true
#| code-fold: false
#| eval: true
#| warning: false

library(tidySEM)
library(rio)
library(sjlabelled)
library(summarytools)
library(ggplot2)
library(tidyr)
```

# Dichotomous indicator example

Then we will import the **political.sav** example data set for analysis. 

```{r}
#| echo: true
#| code-fold: false
#| eval: true
#| warning: false

dat <- import("political.sav")
head(dat)
dim(dat)
```

Looking into the data attributes, we see that the first 5 items are related to their political views, and the following are participant characteristics. with the function ```get_label()``` we see the label attribute extracted from the SPSS data set. And with the function ```get_labels()``` we see the values for each variable

```{r}
#| echo: true
#| code-fold: false
#| eval: true
#| warning: false

get_label(dat)
get_labels(dat)
```

So, here we have a data set about political views from $N=1156$, and with some individual characteristics that can be use as predictors. 

```{r}
#| echo: true
#| code-fold: false
#| eval: true
#| warning: false

freq(dat[,1:5])
```

Looking at the frequency distribution of the 5 indicators of political views, we see that no variable present a major preference for one answer over another. 


## `tidySEM` 

Within `tidySEM` we have functions that will run LCA for a range of $K$ number of classes. But first we need to prepare the data for it, for LCA we need to specify that each variable is of the `ordered` type


```{r}
#| echo: true
#| code-fold: false
#| eval: true
#| warning: false

dat2 <- data.frame(lapply(dat[,1:5], ordered))
descriptives(dat2)

```

Once we have the data in the desired format, we can run the LCA with the `mx_lca()` function, for this function we need to provide the data set (with only categorical indicators), and the number of $K$ classes to estimate, you can also estimate a range of classes like `1:6` to estimate models from 1 to 6 classes. We are also setting a seed so we can better replicate the results. 

```{r}
#| echo: true
#| code-fold: false
#| eval: true
#| warning: false

set.seed(1987) # setting seed 
res1 <- mx_lca(data = dat2, classes = 2)
```

Congratulation! you have run your first LCA. We will look into how to interpret the results in the next sections. 

## Class enumeration

In exploratory LCA, a sequence of models is fitted to the data
with each additional model estimating one more class than the previous
model. These models are then compared and the best solution is selected
as the final class solution. In some cases, prior theory can inform the
researcher about the number of classes to expect.

From a sequence of models, the final class solution is chosen based on
both theoretical and statistical criteria. Theory should drive the
selection of indicator variables, inform the expectations and reflect on
the findings. In addition to this, there are several statistical
criteria to consider in model selection. These include but are not
limited to likelihood ratio tests, information criteria, and the Bayes
factor (Masyn, 2013).

Relative model fit can be examined using the likelihood ratio test. This
is only appropriate when the two models we wish to compare are nested.
The likelihood ratio test statistic is computed as the difference in
maximum log-likelihoods of the two models, with the test degrees of
freedom being the difference in the degrees of freedom of the two
compared models. The test statistic follows the $\chi^2$ distribution,
and we want it to be non-significant in order to give support to the
simpler model. The likelihood ratio test can only compare two nested
models at a time (Collins & Lanza, 2010).

We can provide a range to the `classes` argument to estimate LCAs from 1 to 6 classes

```{r}
#| echo: true
#| code-fold: false
#| eval: true
#| warning: false

res <- mx_lca(data = dat2, classes = 1:6)
is(res)
```

See that we get a convergence message for each model, also it specifies how many errors where there from the 11 times the model was run. Each model is run multiple times with different starting values, this helps to prevent the model to converged on a **local maxima** solution, instead of a **global maxima** solution. 

This provides a list of `Mx` models, so you can access the 5th model like a list in *R*

```{r}
#| echo: true
#| code-fold: false
#| eval: true
#| warning: false

res[[5]]
```


### Model Fit Indices

Fit indices typically used for determining the optimal number of classes
include the Akaike Information Criterion (AIC) and the Bayesian
Information Criterion (BIC). Both information criteria are based on the
-2\*log-likelihood (which is lower for better fitting models), and add a
penalty for the number of parameters (thus incentivizing simpler
models). This helps balance model fit and model complexity. The lower
the value of an information criterion, the better the overall fit of the
model. 

The general objective of the Information Criteria (IC) is to evaluate the model's out-of-sample predictive accuracy, so adjusting for over fitting. Fit measures like $R^2$ evaluate the in-sample predictive accuracy, meaning that they evaluate the models ability to predict the observed outcomes based on the same data that was use to build up the model. These metrics are positively bias, meaning that will present better model fit than it has in reality. While IC corrects for this positive bias by evaluating the models accuracy approximating the out-of-sample predictive accuracy, meaning that it is the ability to predict the outcome for observations that are not part of the training model. Ideally we would estimate the model a lot of time with less observations and predict their scores, but IC's approximate this by their different penalty metrics. For example $R^2$ will increase even if an added predictors are unnecessary, while IC's will show worst fit when a predictor (complexity) is unnecessary (McElreath, 2020)

The BIC applies a stronger penalty for model complexity that
scales logarithmically with the sample size. The literature suggests the
BIC may be the most appropriate information criterion to use for model
comparison (Nylund-Gibson & Choi, 2018; Masyn, 2013)

Information criteria may occasionally contradict each other, so it is
important to identify a suitable strategy to reconcile them. One option
is to select a specific fit index before analyzing the data. Another
option is to always prefer the most parsimonious model that has best fit
according to any of the available fit indices. Yet another option is to
incorporate information from multiple fit indices using the analytic
hierarchy process. Finally, one might make an
elbow plot and compare multiple information criteria (Nylund-Gibson & Choi, 2018).

A disadvantage of the IC's is that we do not have standard errors for them, so we only have the absolute values without a measure of their variability. So, the difference between models IC can be very small. Still indicating that the model with the lower value is "better", but if this difference is  very small can considered them "functionally equal", and you should take into consideration the interpretability of the model. 

LCA studies commonly report -2\*log likelihood of the final class
solution. This is a basic fit measure used to compute most information
criteria. However, since log likelihood is not penalized for model
complexity, it will continuously fall with the addition of more classes.

We can use the `table_fit()` function to calculate a variety of fit indices, for example for the 2 class model

```{r}
#| echo: true
#| code-fold: false
#| eval: true
#| warning: false

table_fit(res[[2]])
```

But we can use the same function to calculate these indices for all models in the list we are creating a data with the number of states and the fit indices, such as log-likelihood, AIC, BIC.

```{r}
#| echo: true
#| code-fold: false
#| eval: true
#| warning: false

fit_ind <- table_fit(res)
fit_ind
```
Looking at AIC and BIC, we see that the model improves (smaller) as the number of classes increases between 1 and 3, and from 4 and above the fit worsen. Then we create the elbow plot. We first structure the fit indices into long format with ```pivot_longer``` function, and then we use ```ggplot2``` to create the elbow plot. 

```{r}
#| echo: true
#| code-fold: false
#| eval: true
#| warning: false

elbow_plot <- fit_ind[ , c("Name","AIC","BIC")] # extract ICs
elbow_plot <- pivot_longer(elbow_plot, cols = c("AIC","BIC"), names_to = "IC", values_to = "Value") # to long format

ggplot(elbow_plot, aes(x = Name, y = Value, group = IC))+
  geom_point(aes(color = IC))+
  geom_line(aes(color = IC))+
  labs(title="Elbow Plot of Information Criteria per Class Solution", 
       x = "Class Solution", y = " Value")+
  theme_minimal()
```

Here we see that a meaningful decrease from 1 to 2 for all indices, them from 2 to 3 it decreases stepper for AIC. For this we would choose the 3 class solution in this case.

### Classification Diagnostics

Best models will divide the sample into subgroups which are internally
homogeneous and externally distinct. Classification diagnostics give us
a way to assess the degree to which this is the case. They are separate
from model fit indices as a model can fit the data well but show poor
latent class separation (Masyn, 2013). Classification diagnostics
should not be used for model selection, but they can be used to
disqualify certain solutions because they are uninterpretable.
Interpretability should always be a consideration when considering
different class solutions (Nylund-Gibson & Choi, 2018)

Four important classification diagnostics are shown here:
(1) the *minimum* and *maximum* percentage of the sample assigned to a
particular *class*, (2) the *range of the posterior class probabilities* by most likely class membership, (3) *entropy*, and (4) *AvePP* average posterior class probability. All three are based on posterior class probabilities.

The posterior class probability is a measure of classification
uncertainty which can be computed for each individual, or averaged for
each latent class. When the posterior class probability is computed for
each individual in the dataset, it represents each person's probability
of belonging to each latent class. For each person, the highest
posterior class probability is then determined and the individual is
assigned to the corresponding class. We want each individual's posterior
class probabilities to be high for one and low for the remaining latent
classes. This is considered a high classification accuracy and means
that the classes are distinct. To obtain posterior class probabilities,
run the `tidySEM` function```class_prob()```. This function produces output comprised of several elements:

```{r}
#| echo: true
#| code-fold: false
#| eval: true
#| warning: false

cl_diag <- class_prob(res[[3]])
```


`$sum.posterior` is a summary table of the posterior class probabilities
indicating what proportion of the data contributes to each class.

```{r}
#| echo: true
#| code-fold: false
#| eval: true
#| warning: false

cl_diag$sum.posterior
```

`$sum.mostlikely` is a summary table of the most likely class membership
based on the highest posterior class probability. From this table, we
compute the minimum and maximum percentage of the sample assigned to a
particular class, , i.e. **n_min** (the smallest class proportion based
on the posterior class probabilities) and **n_max** (the largest class
proportion based on the posterior class probabilities). We are
especially interested in **n_min** as if it is very small and comprised
of few observations, the model for that group might not be locally
identified. It may be impossible to calculate descriptive statistics for
such a small class. Estimating LCA parameters on small subsamples might
lead to bias in the results. Therefore, we advise caution when dealing
with small classes.

```{r}
#| echo: true
#| code-fold: false
#| eval: true
#| warning: false

cl_diag$sum.mostlikely
```

`$mostlikely.class` is a table with rows representing the class the
person was assigned to, and the columns indicating the average posterior
probability. The diagonal represents the probability that observations
in each class will be correctly classified. If any of the values on the
diagonal of this table is low, we might consider not to interpret that
solution. We use the diagonal to compute the range of the
posterior class probabilities by most likely class membership which
consists of the lowest class posterior probability (**prob_min**), and
the highest posterior probability (**prob_max**). Both **prob_min** and
**prob_max** can be used to disqualify certain class solutions, and are
a convenient way to summarize class separation in LCA. We want both
**prob_min** and **prob_max** to be high as that means that for all
classes the people who were assigned to that class have a high
probability of being there. **prob_min** is especially important as it
can diagnose if there is a class with low posterior probabilities which
could make one reconsider that class solution.

```{r}
#| echo: true
#| code-fold: false
#| eval: true
#| warning: false

cl_diag$mostlikely.class
```

`$avg.mostlikely` contains the average posterior probabilities for each
class, for the subset of observations with most likely class of 1:k,
where k is the number of classes.

```{r}
#| echo: true
#| code-fold: false
#| eval: true
#| warning: false

cl_diag$avg.mostlikely
```

`$AvePP` presents the average posterior class probability (mean), and the respective standard deviation, minimum, and maximum.

```{r}
#| echo: true
#| code-fold: false
#| eval: true
#| warning: false

cl_diag$AvePP
```

`$individual` is the individual posterior probability matrix, with
dimensions n (number of cases in the data) x k (number of classes). Additionally it includes the `predicted` class in function of the highest predicted probability.
Individual class probabilities and/or predicted class are often useful for researchers who wish to do follow up analyses. 

```{r}
#| echo: true
#| code-fold: false
#| eval: true
#| warning: false

head(cl_diag$individual)
```

Entropy is a summary measure of posterior class probabilities across
classes and individuals. It ranges from 0 (model classification no
better than random chance) to 1 (perfect classification). As a rule of
thumb, values above .80 are deemed acceptable and those approaching 1
are considered ideal. An appropriate use of entropy is that it can
disqualify certain solutions if class separability is too low. Entropy
was not built for nor should it be used for model selection during class
enumeration [@masyn_latent_2013].

**n_min**, **n_max**, **prob_min**, **prob_max**, and **entropy** and
can be obtained using ```get_fits()```.

```{r}
#| echo: true
#| code-fold: false
#| eval: true
#| warning: false

table_fit(res[[3]])
```



### Interpreting the Final Class Solution

With the `table_prob()` function we can see the estimated probability of answering each option for each item, across classes. 

```{r}
#| echo: true
#| code-fold: false
#| eval: true
#| warning: false

table_prob(res[[3]])
```

Remember we have the mixture probabilities from the classification diagnostics. We see that each class has between 18% and 44% of the sample. 


```{r}
#| echo: true
#| code-fold: false
#| eval: true
#| warning: false

cl_diag$sum.posterior
```

 

We can also plot these response probabilities across classes, by providing our chosen model to the ```plot_prob()``` function from ```tidySEM```.  

```{r}
#| echo: true
#| code-fold: false
#| eval: true
#| warning: false

plot_prob(res[[3]])
```

With this information we would *name* each class and describe the theoretical interpretation of what they mean and what characterize each class. 


# References

Vermunt, J. K., & Magidson, J. (2004). Latent class analysis. In M. Lewis-Beck, A. Bryman, & T. F. Liao (Eds.), The Sage encyclopedia of social sciences research methods (pp. 549-553). Sage. 

Masyn, K. E. (2013). Latent Class Analysis and Finite Mixture Modeling. In P. E. Nathan & T. D. Little (Eds.), The Oxford Handbook of Quantitative Methods: Vol. Volume 2: Statistical Analysis (p. 63). Oxford University Press.

Collins, L. M., & Lanza, S. T. (2010). Latent Class and Latent Transition Analysis: With Applications in the Social, Behavioral, and Health Science. John Wiley & Sons, Inc.

McElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan (2nd ed.). Taylor and Francis, CRC Press.

Nylund-Gibson, K., & Choi, A. Y. (2018). Ten frequently asked questions about latent class analysis. Translational Issues in Psychological Science, 4(4), 440â€“461. https://doi.org/10.1037/tps0000176




