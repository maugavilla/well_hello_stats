---
title: "Multivariate Analysis of Variance (MANOVA)"
author: "Santiago GÃ³mez-Echeverry & Mauricio Garnier-Villarreal"
date: "`r format(Sys.time(), '%d %B, %Y')`"
format: gfm
toc: true
toc-depth: 5
code-copy: true
execute:
  cache: true
---

# Introduction

Often, as social scientists we are interested in the effect that treatment has in more than one dependent variable (DV). A naive approach towards this problem would be to perform a separate analysis for each DV. However, by doing this we would ignore that the two outcome measures might be correlated. To analyze multiple DV's while considering their potential relationship we will use a technique called Multivariate Analysis of Variance (MANOVA). In this tutorial, we'll delve into questions such as what a MANOVA analysis is, when should we use it, and how it can inform social science research. Let's dive in.

MANOVA is an extension of ANOVA, with which the researchers can estimate the meandifference between groups in terms of two or more outcome measures. This method has been applied extensively in the fields of psychology, medicine and education. Note that we will only deal with between variation, that is differences between groups and measurements a one particular time point. However, the MANOVA method can be extended to include both between and within variation.

# Set up the R Session

When we start working in R, we always need to setup our session. For this we need to set our working directory. You should set it for the folder that holds the data set. 

```{r, eval=F, error=F}
setwd("~path_to_your_file")
```

The next step for setting up our session will be to load the packages that we will be using. We will use a packages like `dplyr` for data management, as well as the `rio` package for importing our data. Additionally, we need the `reshape2` to reshape our data. Finally, the packages `afex`, and `marginaleffects` are used for conducting our analyses. Note that you potentially need to install some of these packages, however.

```{r, eval=T, message=F, warning=F, results=F}
library(dplyr)
library(reshape2)
library(marginaleffects)
library(afex)
library(sjlabelled)
library(effectsize)
library(rio)
library(bruceR)
library(corrplot)
library(car)
library(heplots)
library(candisc)
```

# Import the Dataset

We are importing a `.xlsx` file with the OECD Regional well-being data set (\href{https://stats.oecd.org/Index.aspx?DataSetCode=RWB}{https://stats.oecd.org/Index.aspx?DataSetCode=RWB}). This data contains results for eleven (11) sub-dimension of well-being for different regions of the European continent. The 11 sub-dimensions comprise four (4) dimensions: **material conditions** (income, jobs, and housing), **quality of life** (education, health, environment, safety, and access to services), and **subjective well-being** (social network support and life satisfaction). In most cases, the indicators selected to measure these dimensions is a combination of people's attributes and their living conditions, available over two different periods (2000 and 2014). The data includes 208 regional-level observations and 17 variables.

```{r, eval=T}
dat <- import("OECD.xlsx")
head(dat)
dim(dat)
```

Or if you want to pull it from the github site

```{r, eval=F}
dat <- import("https://github.com/maugavilla/well_hello_stats/raw/main/tutorials/OECD.xlsx")
head(dat)
dim(dat)
```

Before we do any arrangements with the data, it would be good if we have a look at the variables that we have with some descriptive statistics. Remember, this only makes sense for variables that are *numeric*. If we wanted to see how factors or characters are distributed in the data we would require a different approach, like frequency tables. To get the descriptive statistics we will use the `Describe` function from the `bruceR` package.

```{r, eval=T, warning = F}
Describe(dat[,-c(1,2,3,4)]) # note that we're excluding the first 4 columns as they contain character values.
```

Additionally, we can see how the variables are related to each other. To do this we will use a correlation matrix, also referred as correlogram. We would expect that the variables that are part of the same well-being dimension have a *strong* correlation, whether it is positive or negative.

```{r, eval=T, warning = F}
cor_m <- cor(dat[,-c(1,2,3,4)])
corrplot(cor_m, type = 'lower', order = 'alphabet', method = 'color', 
         tl.cex = 0.5, tl.col = 'black', addCoef.col = 1, number.cex = 0.5) # this line sets the size and color of the text in the plot
```

Now that we have already explored how the data looks like, we can continue with the data arrangement. For this exercise we will compare only three groups that are defined from the variable continent. We will compare Western, Eastern, and Central Europe. Besides, we will only use a set of variables from the original pool. We will concentrate in the variables `life` that measures life expectancy, `vote` that measures political participation, and `education` which evaluates the overall quality of the education.

```{r, eval=T, warning = F}
dat2 <- filter(dat,(continent=="Eastern Europe" | continent == "Central Europe" | continent == "Western Europe"))
dat_lq <- dat2[,c('region','continent','life', 'vote', 'education')]
```

Before we start with the MANOVA analysis, let's see how these variables behave within our groups. To evaluate this we will get the means of every outcome by group and plot them in bar plot.
First, we create an aggregate (grouped) dataset ```dat_lq_agg``` with the means of our outcome variables per continent. 
This dataset we then reshape into a long format (```dat_lq_long```) and use this to create a plot. 

```{r, eval=T, warning = F}
dat_lq_agg <- aggregate(dat_lq[,-c(1,2)], by = list(dat_lq$continent), mean)       
dat_lq_agg

dat_lq_long <- melt(dat_lq_agg, id = "Group.1", value.name = "value", variable.name = "dimension")
head(dat_lq_long)

ggplot(data = dat_lq_long, aes(x = dimension, y =value, fill = dimension)) +
  geom_bar(stat = "identity") + 
  facet_wrap(~Group.1, scales = 'free_y') + # here, we group per region and set the scales of the y-axis as not-fixed
    theme_bw() +
    theme(axis.title.x=element_blank(), # with these lines, we remove the text from the x-axis
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank())
```

# Perform Multivariate Analysis of Variance

Now, it is time to actually perform the MANOVA using the function `Anova` from the `car` package that we loaded earlier.

First, we will define a linear model with the function `lm` specifying all our variables. Keep in mind that now that we have multiple DV's we need to refer to them as block in the model by employing the `cbind`. Once we have defined our model we can perform the MANOVA analysis, and keep the results in a new object.

```{r, eval=TRUE, warning=FALSE}

dat_lq
mod <- lm(cbind(life, vote, education) ~ continent, data = dat_lq)
maov <- summary(Anova(mod), univariate = TRUE, digits = 3)
maov
```

The output `summary(model)` present different sections. First, we have the $SSCP_{R}$ and $SSCP_{E}$ matrices. These matrices contain the sum of squares of each variable in the diagonals and the cross-products outside of it. These matrices are the ones employed in the construction of the following part of the output that is the multivariate tests. The output presents the four typical statistics employed to assess the significance of the intervention in MANOVA: **Pillai's trace, Wilks lambda, Hotelling-Lawley T, and Roy's largest root**.

## Effect size

We also need to describe the results in function of measures of effect size. For ANOVA family of analysis, we recommend to use $\eta^2_f$ and $\omega^2_f$. These measures estimate the proportion of variance explained by each predictor (similar to $R^2$). Where $\eta^2_f$ is more positively bias (similar to $R^2$), and $\omega^2_f$ is a more conservative measure.

When you have multiple predictors, you will also see the **partial** version of these measures ($\eta^2_p$ and $\omega^2_p$). These will most commonly present higher effect sizes, as they are the proportion of explained variance **that is not predicted by any other predictors**. So, will show higher effect sizes because they are not in function of the total variance of the outcome, but the residual variance.

To estimate these measure we will use the `effectsize` package functions, just provide the MANOVA model to the respective functions, notice that we set `partial = FALSE` to estimate the full measure instead of partial one (in this case they will be the same, as we have only one predictor)

```{r}
eta_squared(mod, partial = F)
omega_squared(mod, partial = F)
```

When the model has only one predictor, these measures are equivalent to the model $R^2$. In this case the show that for `life` around a 49% ($\eta^2_f = 0.49$) of the variance is explained by the `continent` variable, around 26\% for `vote`, and 8\% for `education`.

## Homogeneity of Variances

Since we have between variation, we will assume that the variance of the outcome variables is the same across groups. This assumption is what we have referred as homogeneity of variances. However, we can't use the same test as before, because we're dealing with *matrices* in this situation. The test that we will employ in this case is **Box's M test**, which uses a chi-squared approximation to perform hypotheses testing. As per usual, the null hypothesis of this test is that the variances/covariances are the same across the groups. Keep in mind that this test is not reliable when the sizes of the groups are very irregular.

```{r}
boxM(dat_lq[,c('life', 'vote', 'education')], dat_lq$continent)
```

As you see, from the function `boxM` we get the chi-squared, the degrees of freedom and the p-value. With these results $\chi^{2}(df= 12)=127.93, p < 0.001$ we reject the null hypothesis. This implies that we do *not* have homogeneity of variances/covariances. In this situation, there are two possible approaches: one is to **use a higher significance level** in the hypothesis testing, and the other is to **transform the dependent variable** so that we don't observe these difference on variances. Keep in mind that transforming the data does not imply arbitrarily editing the data, but rather rescale the information in a sensible way (e.g., by standardizing the variables).

## HE Plots
A nice way to evaluate the MANOVA model is through **Hypothesis-Error (HE)** plots. This plots show two DVs simultaneously, and often separate the diverse sources of variance with ellipses. The center of the ellipses refer to means in the two variables, the **height** of the ellipses denotes shows the **standard deviation** of the variable in the **vertical axis**, and the **width** the one in the **horizontal axis**. The **inclination** of the the ellipses show the **relationship** between the two variables.

Let us see an HE plot with the different groups that we have.

```{r, eval=TRUE, warning=FALSE}
cols = c(scales::hue_pal()(3), "black") # here, we create an object storing four different colors that we use in the plot below
covEllipses(dat_lq[,c('life', 'vote', 'education')], dat_lq$continent, 
            col=cols, fill=c(rep(FALSE,3), TRUE), fill.alpha=.05) # this line deals with the plot esthetics
```
If the ellipses of different groups **overlap** significantly, it suggests that the groups are **not very different** with respect to the dependent variables. The **further apart** the centroids (multivariate means) of the ellipses are, the **more distinct** the groups are likely to be.

This was a good taste of how the plots work. Now, let's dive into the plot with the model and the error.

```{r, eval=TRUE, warning=FALSE}
heplot(mod, fill=TRUE, fill.alpha=0.1)
```

The **centroid** in the middle represents the multivariate mean of the two dependent variables. The **shape** and **orientation** of the ellipses are indicative of the correlation between the outcome variables: we observe a **positive correlation** between ```life``` and ```vote```. A perfect circle would signify no correlation, a(n almost) straight line would signify a correlation of â1.
Given that the ellipse from our predictor (continent) goes outside of the error ellipse, we can state that there *is* a significant effect. However, it's not clear whether this effect is large or small, since the **length of the ellipse is dependent on the scale of the variables**. A way to assess if the *real* size of the effect visually, is to add the argument `size="effect"`.

```{r, eval=TRUE, warning=FALSE}
heplot(mod, fill=TRUE, fill.alpha=0.1, size = "effect")
```

We have only compared two variables. Now, let's compare them all.

```{r, eval=TRUE, warning=FALSE}
pairs(mod, variables=c('life', 'vote', 'education'))
```
We notice that group means for ```life``` and ```vote``` are positively correlated (we established that above), but there are negative correlations between the group means of both ```vote``` and ```life``` on ```education```. 

## LDA

Often, MANOVA analyses are followed up with a Linear Discriminant Analysis (LDA). The idea of LDA is to see how much of the variables that before were our DVs could **explain** the belonging to the groups. That is, we're **reversing the research question**. 
Additionally, we are interested in seeing if our variables capture one or several **underlying dimensions**. As you saw in class, this might make sense, since we could have a strong correlation between the observed variables. To perform our analysis we will use the `candisc` function. 

```{r, warning = F}
cda <- candisc(mod, term = "continent")
summary(cda)
```

The `summary`results have three parts. The first part corresponds to the **eigenvalues**, which show how much of the total variance is explained by each dimension (or as we referred to them in class: **variates**). The second part shows the means of variates for each group, and the last part of the output shows the standardized coefficients. These coeffcients are the ones that are employed to construct the variates. The coefficients, both standardized and *raw* are be stored in this new object:

```{r, warning = F}
cda$coeffs.raw
cda$coeffs.std
```
```life```, ```vote``` and ```education``` are negatively related to the first variate. In terms of the second variate, ```life``` and ```education``` are positively related to it and ```vote``` is negatively related to it.

To end, let's see the behavior of the groups and the observed variables in the two underlying variates that we obtain from the LDA. Note that the ellipses are perfect circles, indicating that there is **no correlation** between the two variates. 

```{r, warning = F}
col <- scales::hue_pal()(3) #again, this row sets the colors for the plot below
plot(cda, col=col, 
     ellipse=TRUE, var.col = "black")
```
